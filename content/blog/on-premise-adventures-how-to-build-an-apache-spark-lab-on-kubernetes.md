---
title: "On-Premise Adventures: How to build an Apache Spark lab on Kubernetes"
date: 2021-06-15T16:51:59.069Z
author: Don Wake
authorimage: /img/Avatar1.svg
---
Apache Spark™ is an awesomely powerful developer tool for finding the value in your data.  I highly recommend you check out this Hewlett Packard Enterprise (HPE) white paper for some background – [Apache Spark 3 on HPE Ezmeral](https://www.hpe.com/psnow/doc/a50004177enw). In this post, I’m going to explain how I deployed Apache Spark in my own on-premises HPE Ezmeral Container Platform-managed lab so that I could try Apache Spark out for myself. 

### We need a story first, right?

Suppose your story is similar to mine: I am a data scientist and I work for ACME Windmills Incorporated – or just ACME, for short.  ACME owns and operates windmills that generate power. They have multiple sites, such as one in Australia and another one in Southern California. I’ve been asked to predict how much power will be generated by these various sites.  

### What tools do I have at my disposal?

ACME handed me the ultimate toolbox to set me on my Apache Spark journey, a toolbox called the [HPE Ezmeral Software Platform](https://www.hpe.com/us/en/ezmeral.html). HPE Ezmeral software provides me with an “all of the above” approach to deployment and management of my data, apps, and the compute & storage resources that run it all - from anywhere. I find that I can use this toolbox to:

* modernize legacy apps, and manage those apps alongside cloud-native apps
* use existing data lakes alongside HPE Ezmeral Data Fabric 

And, I can even get all this from HPE as a service!

I’ve decided I want to make this an Apache Spark job, because it will use machine learning to help me make the predictions. For this particular job, I pulled out the [HPE Ezmeral Container Platform](http://www.hpe.com/containerplatform) from my toolbox for application management – it allows me to specifically run my machine learning (ML) jobs, including Apache Spark, on Kubernetes. It also gave me the ability to easily connect ACME’s [HPE Ezmeral Data Fabric](https://assets.ext.hpe.com/is/content/hpedam/a00110846enw) to my Apache Spark jobs. I have been given an awesome set of tools!

### What exactly am I going to build?

As mentioned above, ACME needs to predict power output from their windmills, so they can make intelligent decisions about where to add more windmills and how to optimize the windmills they already have. This is a job for Apache Spark on Kubernetes on HPE Ezmeral! My overall architecture will look like this:

\[image1]

As the figure above indicates, I am first going to build a Kubernetes Cluster. In that cluster, I will deploy an Apache Spark Operator. The Apache Spark Operator is the glue that allows you to run Apache Spark within a containerized environment, such as a Kubernetes Cluster. Once I have the Kubernetes Cluster, I can then carve out a Kubernetes Namespace and deploy some machine learning applications, such as a containerized Jupyter Hub server running a Jupyter Notebook. I will use that Jupyter Notebook to run my Apache Spark jobs.

The code running inside the Jupyter Notebook will make an API call to the Apache Spark Operator using an API server called “Livy”. Livy will ask Apache Spark to create an Apache Spark session so I can retrieve value from the data. Apache Spark will be able to ingest data from the Australia core deployment of HPE Ezmeral Data Fabric using an HPE Ezmeral Container Platform feature called a DataTap (more on that below).

## Let’s Build it!

If you are a data scientist or data engineer and you really don’t want to know how all these back-end system administrator tasks are done, then I highly recommend you skip to the **Now we get to run our Apache Spark jobs!** section. I’d definitely put this section firmly in the infrastructure person category.

First, I log into my HPE Ezmeral Container Platform WebUI. My organization has already setup Active Directory integration to use with the HPE Ezmeral Container Platform WebUI.

\[Image2]

Below, on the Kubernetes Dashboard Web UI page, you can see a list of options in the left hand pane.  One of those options is **Clusters** found under the **Kubernetes** section in the upper left hand portion of the panel on the left side of the screen. From my dashboard, I click on my **Cluster’s** option on the left under **Kubernetes**.

\[Image3]

Next, in the **Kubernetes Clusters** window, I see I have a single cluster called DataFabricOnK8s.  This cluster is running a version of HPE Ezmeral Data Fabric that runs within a Kubernetes Cluster. This is handy since now all of my Kubernetes namespaces and other clusters will get the same enterprise-grade storage features available in a Bare Metal deployment of HPE Ezmeral Data Fabric. I will create a new Kubernetes namespace from this cluster. To do that, I click on **Tenants** on the left. Tenants, as the name implies, are how we organize multiple Kubernetes namespaces in the HPE Ezmeral Container Platform. Or, in other words, how we help you manage ***multi-tenancy***.

\[Image4]